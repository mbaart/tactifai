{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5906bfd3",
      "metadata": {
        "id": "5906bfd3"
      },
      "source": [
        "# X3D\n",
        "\n",
        "*Author: FAIR PyTorchVideo*\n",
        "\n",
        "**X3D networks pretrained on the Kinetics 400 dataset**\n",
        "\n",
        "\n",
        "### Example Usage\n",
        "\n",
        "#### Imports\n",
        "\n",
        "Load the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "d75c07ce",
      "metadata": {
        "id": "d75c07ce"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in C:\\Users\\micha/.cache\\torch\\hub\\facebookresearch_pytorchvideo_main\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "# Choose the `x3d_s` model\n",
        "model_name = 'x3d_s'\n",
        "model = torch.hub.load('facebookresearch/pytorchvideo', model_name, pretrained=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d543f671",
      "metadata": {
        "id": "d543f671"
      },
      "source": [
        "Import remaining functions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "17a1c222",
      "metadata": {
        "id": "17a1c222"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\micha\\Git\\tactifAI\\.venv-torch251\\Lib\\site-packages\\torchvision\\transforms\\_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
            "  warnings.warn(\n",
            "c:\\Users\\micha\\Git\\tactifAI\\.venv-torch251\\Lib\\site-packages\\torchvision\\transforms\\_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import urllib\n",
        "from pytorchvideo.data.encoded_video import EncodedVideo\n",
        "\n",
        "from torchvision.transforms import Compose, Lambda\n",
        "from torchvision.transforms._transforms_video import (\n",
        "    CenterCropVideo,\n",
        "    NormalizeVideo,\n",
        ")\n",
        "from pytorchvideo.transforms import (\n",
        "    ApplyTransformToKey,\n",
        "    ShortSideScale,\n",
        "    UniformTemporalSubsample\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48133d16",
      "metadata": {
        "id": "48133d16"
      },
      "source": [
        "#### Setup\n",
        "\n",
        "Set the model to eval mode and move to desired device."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "f39e4e44",
      "metadata": {
        "id": "f39e4e44"
      },
      "outputs": [],
      "source": [
        "# Set to GPU or CPU\n",
        "device = \"cuda\"\n",
        "model = model.eval()\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4aa1ec11",
      "metadata": {
        "id": "4aa1ec11"
      },
      "source": [
        "Download the id to label mapping for the Kinetics 400 dataset on which the torch hub models were trained. This will be used to get the category label names from the predicted class ids."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "006d95b5",
      "metadata": {
        "id": "006d95b5"
      },
      "outputs": [],
      "source": [
        "json_url = \"https://dl.fbaipublicfiles.com/pyslowfast/dataset/class_names/kinetics_classnames.json\"\n",
        "json_filename = \"kinetics_classnames.json\"\n",
        "try: urllib.URLopener().retrieve(json_url, json_filename)\n",
        "except: urllib.request.urlretrieve(json_url, json_filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "c04d8889",
      "metadata": {
        "id": "c04d8889"
      },
      "outputs": [],
      "source": [
        "with open(json_filename, \"r\") as f:\n",
        "    kinetics_classnames = json.load(f)\n",
        "\n",
        "# Create an id to label name mapping\n",
        "kinetics_id_to_classname = {}\n",
        "for k, v in kinetics_classnames.items():\n",
        "    kinetics_id_to_classname[v] = str(k).replace('\"', \"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a03f088",
      "metadata": {
        "id": "7a03f088"
      },
      "source": [
        "#### Define input transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "29d5e6d3",
      "metadata": {
        "id": "29d5e6d3"
      },
      "outputs": [],
      "source": [
        "mean = [0.45, 0.45, 0.45]\n",
        "std = [0.225, 0.225, 0.225]\n",
        "frames_per_second = 30\n",
        "model_transform_params  = {\n",
        "    \"x3d_xs\": {\n",
        "        \"side_size\": 182,\n",
        "        \"crop_size\": 182,\n",
        "        \"num_frames\": 4,\n",
        "        \"sampling_rate\": 12,\n",
        "    },\n",
        "    \"x3d_s\": {\n",
        "        \"side_size\": 182,\n",
        "        \"crop_size\": 182,\n",
        "        \"num_frames\": 13,\n",
        "        \"sampling_rate\": 6,\n",
        "    },\n",
        "    \"x3d_m\": {\n",
        "        \"side_size\": 256,\n",
        "        \"crop_size\": 256,\n",
        "        \"num_frames\": 16,\n",
        "        \"sampling_rate\": 5,\n",
        "    }\n",
        "}\n",
        "\n",
        "# Get transform parameters based on model\n",
        "transform_params = model_transform_params[model_name]\n",
        "\n",
        "# Note that this transform is specific to the slow_R50 model.\n",
        "transform =  ApplyTransformToKey(\n",
        "    key=\"video\",\n",
        "    transform=Compose(\n",
        "        [\n",
        "            UniformTemporalSubsample(transform_params[\"num_frames\"]),\n",
        "            Lambda(lambda x: x/255.0),\n",
        "            NormalizeVideo(mean, std),\n",
        "            ShortSideScale(size=transform_params[\"side_size\"]),\n",
        "            CenterCropVideo(\n",
        "                crop_size=(transform_params[\"crop_size\"], transform_params[\"crop_size\"])\n",
        "            )\n",
        "        ]\n",
        "    ),\n",
        ")\n",
        "\n",
        "# The duration of the input clip is also specific to the model.\n",
        "clip_duration = (transform_params[\"num_frames\"] * transform_params[\"sampling_rate\"])/frames_per_second"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "920d43ff",
      "metadata": {
        "id": "920d43ff"
      },
      "source": [
        "#### Run Inference\n",
        "\n",
        "Download an example video."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "aea6220e",
      "metadata": {
        "id": "aea6220e"
      },
      "outputs": [],
      "source": [
        "url_link = \"https://dl.fbaipublicfiles.com/pytorchvideo/projects/archery.mp4\"\n",
        "video_path = 'archery.mp4'\n",
        "try: urllib.URLopener().retrieve(url_link, video_path)\n",
        "except: urllib.request.urlretrieve(url_link, video_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d84f4ae",
      "metadata": {},
      "source": [
        "Using soccer video."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "dbbad8a9",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "video_path = os.path.join(os.getcwd(), \"78-trim.mp4\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33fc72b7",
      "metadata": {
        "id": "33fc72b7"
      },
      "source": [
        "Load the video and transform it to the input format required by the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "c562af37",
      "metadata": {
        "id": "c562af37"
      },
      "outputs": [],
      "source": [
        "# Select the duration of the clip to load by specifying the start and end duration\n",
        "# The start_sec should correspond to where the action occurs in the video\n",
        "start_sec = 0\n",
        "end_sec = start_sec + clip_duration\n",
        "\n",
        "# Initialize an EncodedVideo helper class and load the video\n",
        "video = EncodedVideo.from_path(video_path)\n",
        "\n",
        "# Load the desired clip\n",
        "video_data = video.get_clip(start_sec=start_sec, end_sec=end_sec)\n",
        "\n",
        "# Apply a transform to normalize the video input\n",
        "video_data = transform(video_data)\n",
        "\n",
        "# Move the inputs to the desired device\n",
        "inputs = video_data[\"video\"]\n",
        "inputs = inputs.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "648b7c9d",
      "metadata": {
        "id": "648b7c9d"
      },
      "source": [
        "#### Get Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "34232a48",
      "metadata": {
        "id": "34232a48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 5 predicted labels: shooting goal (soccer), pumping fist, hurling (sport), headbutting, applauding\n"
          ]
        }
      ],
      "source": [
        "# Pass the input clip through the model\n",
        "preds = model(inputs[None, ...])\n",
        "\n",
        "# Get the predicted classes\n",
        "post_act = torch.nn.Softmax(dim=1)\n",
        "preds = post_act(preds)\n",
        "pred_classes = preds.topk(k=5).indices[0]\n",
        "\n",
        "# Map the predicted classes to the label names\n",
        "pred_class_names = [kinetics_id_to_classname[int(i)] for i in pred_classes]\n",
        "print(\"Top 5 predicted labels: %s\" % \", \".join(pred_class_names))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6573277",
      "metadata": {
        "id": "b6573277"
      },
      "source": [
        "### Model Description\n",
        "X3D model architectures are based on [1] pretrained on the Kinetics dataset.\n",
        "\n",
        "| arch | depth | frame length x sample rate | top 1 | top 5 | Flops (G) | Params (M) |\n",
        "| --------------- | ----------- | ----------- | ----------- | ----------- | ----------- |  ----------- | ----------- |\n",
        "| X3D      | XS    | 4x12                       | 69.12 | 88.63 | 0.91      | 3.79     |\n",
        "| X3D      | S     | 13x6                       | 73.33 | 91.27 | 2.96      | 3.79     |\n",
        "| X3D      | M     | 16x5                       | 75.94 | 92.72 | 6.72      | 3.79     |\n",
        "\n",
        "\n",
        "### References\n",
        "[1] Christoph Feichtenhofer, \"X3D: Expanding Architectures for\n",
        "    Efficient Video Recognition.\" https://arxiv.org/abs/2004.04730"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv-torch251",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
