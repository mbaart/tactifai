{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1823c232",
      "metadata": {
        "id": "1823c232"
      },
      "source": [
        "### This notebook is optionally accelerated with a GPU runtime.\n",
        "### If you would like to use this acceleration, please select the menu option \"Runtime\" -> \"Change runtime type\", select \"Hardware Accelerator\" -> \"GPU\" and click \"SAVE\"\n",
        "\n",
        "----------------------------------------------------------------------\n",
        "\n",
        "# YOLOP\n",
        "\n",
        "*Author: Hust Visual Learning Team*\n",
        "\n",
        "**YOLOP pretrained on the BDD100K dataset**\n",
        "\n",
        "## Before You Start\n",
        "To install YOLOP dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "f0fccc99",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:\\Users\\micha\\Git\\tactifAI\\notebooks\\examples\\yolop\\requirements.txt\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "requirements_path = os.path.join(os.getcwd(), 'requirements.txt')\n",
        "print(requirements_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "66dda2dd",
      "metadata": {
        "id": "66dda2dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scipy in c:\\users\\micha\\git\\tactifai\\.venv-torch251\\lib\\site-packages (from -r requirements.txt (line 1)) (1.14.1)\n",
            "Requirement already satisfied: tqdm in c:\\users\\micha\\git\\tactifai\\.venv-torch251\\lib\\site-packages (from -r requirements.txt (line 2)) (4.67.1)\n",
            "Requirement already satisfied: yacs in c:\\users\\micha\\git\\tactifai\\.venv-torch251\\lib\\site-packages (from -r requirements.txt (line 3)) (0.1.8)\n",
            "Requirement already satisfied: Cython in c:\\users\\micha\\git\\tactifai\\.venv-torch251\\lib\\site-packages (from -r requirements.txt (line 4)) (3.0.11)\n",
            "Requirement already satisfied: matplotlib>=3.2.2 in c:\\users\\micha\\git\\tactifai\\.venv-torch251\\lib\\site-packages (from -r requirements.txt (line 5)) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\micha\\git\\tactifai\\.venv-torch251\\lib\\site-packages (from -r requirements.txt (line 6)) (2.2.0)\n",
            "Requirement already satisfied: opencv-python>=4.1.2 in c:\\users\\micha\\git\\tactifai\\.venv-torch251\\lib\\site-packages (from -r requirements.txt (line 7)) (4.10.0.84)\n",
            "Requirement already satisfied: Pillow in c:\\users\\micha\\git\\tactifai\\.venv-torch251\\lib\\site-packages (from -r requirements.txt (line 8)) (11.0.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\micha\\git\\tactifai\\.venv-torch251\\lib\\site-packages (from -r requirements.txt (line 9)) (6.0.2)\n",
            "Requirement already satisfied: tensorboardX in c:\\users\\micha\\git\\tactifai\\.venv-torch251\\lib\\site-packages (from -r requirements.txt (line 11)) (2.6.2.2)\n",
            "Requirement already satisfied: seaborn in c:\\users\\micha\\git\\tactifai\\.venv-torch251\\lib\\site-packages (from -r requirements.txt (line 12)) (0.13.2)\n",
            "Requirement already satisfied: prefetch_generator in c:\\users\\micha\\git\\tactifai\\.venv-torch251\\lib\\site-packages (from -r requirements.txt (line 13)) (1.0.3)\n",
            "Requirement already satisfied: imageio in c:\\users\\micha\\git\\tactifai\\.venv-torch251\\lib\\site-packages (from -r requirements.txt (line 14)) (2.36.1)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\micha\\git\\tactifai\\.venv-torch251\\lib\\site-packages (from -r requirements.txt (line 15)) (1.6.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\micha\\git\\tactifai\\.venv-torch251\\lib\\site-packages (from tqdm->-r requirements.txt (line 2)) (0.4.6)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\micha\\git\\tactifai\\.venv-torch251\\lib\\site-packages (from matplotlib>=3.2.2->-r requirements.txt (line 5)) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\micha\\git\\tactifai\\.venv-torch251\\lib\\site-packages (from matplotlib>=3.2.2->-r requirements.txt (line 5)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\micha\\git\\tactifai\\.venv-torch251\\lib\\site-packages (from matplotlib>=3.2.2->-r requirements.txt (line 5)) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\micha\\git\\tactifai\\.venv-torch251\\lib\\site-packages (from matplotlib>=3.2.2->-r requirements.txt (line 5)) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\micha\\git\\tactifai\\.venv-torch251\\lib\\site-packages (from matplotlib>=3.2.2->-r requirements.txt (line 5)) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\micha\\git\\tactifai\\.venv-torch251\\lib\\site-packages (from matplotlib>=3.2.2->-r requirements.txt (line 5)) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\micha\\git\\tactifai\\.venv-torch251\\lib\\site-packages (from matplotlib>=3.2.2->-r requirements.txt (line 5)) (2.9.0.post0)\n",
            "Requirement already satisfied: protobuf>=3.20 in c:\\users\\micha\\git\\tactifai\\.venv-torch251\\lib\\site-packages (from tensorboardX->-r requirements.txt (line 11)) (5.29.1)\n",
            "Requirement already satisfied: pandas>=1.2 in c:\\users\\micha\\git\\tactifai\\.venv-torch251\\lib\\site-packages (from seaborn->-r requirements.txt (line 12)) (2.2.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\micha\\git\\tactifai\\.venv-torch251\\lib\\site-packages (from scikit-learn->-r requirements.txt (line 15)) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\micha\\git\\tactifai\\.venv-torch251\\lib\\site-packages (from scikit-learn->-r requirements.txt (line 15)) (3.5.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\micha\\git\\tactifai\\.venv-torch251\\lib\\site-packages (from pandas>=1.2->seaborn->-r requirements.txt (line 12)) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\micha\\git\\tactifai\\.venv-torch251\\lib\\site-packages (from pandas>=1.2->seaborn->-r requirements.txt (line 12)) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\micha\\git\\tactifai\\.venv-torch251\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.2.2->-r requirements.txt (line 5)) (1.17.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5cc6d412",
      "metadata": {
        "id": "5cc6d412"
      },
      "source": [
        "## YOLOP: You Only Look Once for Panoptic driving Perception\n",
        "\n",
        "### Model Description\n",
        "\n",
        "<img width=\"800\" alt=\"YOLOP Model\" src=\"https://github.com/hustvl/YOLOP/raw/main/pictures/yolop.png\">\n",
        "&nbsp;\n",
        "\n",
        "- YOLOP is an efficient multi-task network that can jointly handle three crucial tasks in autonomous driving: object detection, drivable area segmentation and lane detection. And it is also the first to reach real-time on embedded devices while maintaining state-of-the-art level performance on the **BDD100K** dataset.\n",
        "\n",
        "\n",
        "### Results\n",
        "\n",
        "#### Traffic Object Detection Result\n",
        "\n",
        "| Model          | Recall(%) | mAP50(%) | Speed(fps) |\n",
        "| -------------- | --------- | -------- | ---------- |\n",
        "| `Multinet`     | 81.3      | 60.2     | 8.6        |\n",
        "| `DLT-Net`      | 89.4      | 68.4     | 9.3        |\n",
        "| `Faster R-CNN` | 77.2      | 55.6     | 5.3        |\n",
        "| `YOLOv5s`      | 86.8      | 77.2     | 82         |\n",
        "| `YOLOP(ours)`  | 89.2      | 76.5     | 41         |\n",
        "\n",
        "#### Drivable Area Segmentation Result\n",
        "\n",
        "| Model         | mIOU(%) | Speed(fps) |\n",
        "| ------------- | ------- | ---------- |\n",
        "| `Multinet`    | 71.6    | 8.6        |\n",
        "| `DLT-Net`     | 71.3    | 9.3        |\n",
        "| `PSPNet`      | 89.6    | 11.1       |\n",
        "| `YOLOP(ours)` | 91.5    | 41         |\n",
        "\n",
        "#### Lane Detection Result\n",
        "\n",
        "| Model         | mIOU(%) | IOU(%) |\n",
        "| ------------- | ------- | ------ |\n",
        "| `ENet`        | 34.12   | 14.64  |\n",
        "| `SCNN`        | 35.79   | 15.84  |\n",
        "| `ENet-SAD`    | 36.56   | 16.02  |\n",
        "| `YOLOP(ours)` | 70.50   | 26.20  |\n",
        "\n",
        "#### Ablation Studies 1: End-to-end v.s. Step-by-step\n",
        "\n",
        "| Training_method | Recall(%) | AP(%) | mIoU(%) | Accuracy(%) | IoU(%) |\n",
        "| --------------- | --------- | ----- | ------- | ----------- | ------ |\n",
        "| `ES-W`          | 87.0      | 75.3  | 90.4    | 66.8        | 26.2   |\n",
        "| `ED-W`          | 87.3      | 76.0  | 91.6    | 71.2        | 26.1   |\n",
        "| `ES-D-W`        | 87.0      | 75.1  | 91.7    | 68.6        | 27.0   |\n",
        "| `ED-S-W`        | 87.5      | 76.1  | 91.6    | 68.0        | 26.8   |\n",
        "| `End-to-end`    | 89.2      | 76.5  | 91.5    | 70.5        | 26.2   |\n",
        "\n",
        "#### Ablation Studies 2: Multi-task v.s. Single task\n",
        "\n",
        "| Training_method | Recall(%) | AP(%) | mIoU(%) | Accuracy(%) | IoU(%) | Speed(ms/frame) |\n",
        "| --------------- | --------- | ----- | ------- | ----------- | ------ | --------------- |\n",
        "| `Det(only)`     | 88.2      | 76.9  | -       | -           | -      | 15.7            |\n",
        "| `Da-Seg(only)`  | -         | -     | 92.0    | -           | -      | 14.8            |\n",
        "| `Ll-Seg(only)`  | -         | -     | -       | 79.6        | 27.9   | 14.8            |\n",
        "| `Multitask`     | 89.2      | 76.5  | 91.5    | 70.5        | 26.2   | 24.4            |\n",
        "\n",
        "**Notes**:\n",
        "\n",
        "- In table 4, E, D, S and W refer to Encoder, Detect head, two Segment heads and whole network. So the Algorithm (First, we only train Encoder and Detect head. Then we freeze the Encoder and Detect head as well as train two Segmentation heads. Finally, the entire network is trained jointly for all three tasks.) can be marked as ED-S-W, and the same for others.\n",
        "\n",
        "### Visualization\n",
        "\n",
        "#### Traffic Object Detection Result\n",
        "\n",
        "<img width=\"800\" alt=\"Traffic Object Detection Result\" src=\"https://github.com/hustvl/YOLOP/raw/main/pictures/detect.png\">\n",
        "&nbsp;\n",
        "\n",
        "#### Drivable Area Segmentation Result\n",
        "\n",
        "<img width=\"800\" alt=\"Drivable Area Segmentation Result\" src=\"https://github.com/hustvl/YOLOP/raw/main/pictures/da.png\">\n",
        "&nbsp;\n",
        "\n",
        "#### Lane Detection Result\n",
        "\n",
        "<img width=\"800\" alt=\"Lane Detection Result\" src=\"https://github.com/hustvl/YOLOP/raw/main/pictures/ll.png\">\n",
        "&nbsp;\n",
        "\n",
        "**Notes**:\n",
        "\n",
        "- The visualization of lane detection result has been post processed by quadratic fitting.\n",
        "\n",
        "### Deployment\n",
        "\n",
        "Our model can reason in real-time on **Jetson Tx2**, with **Zed Camera** to capture image. We use **TensorRT** tool for speeding up. We provide code for deployment and reasoning of model in [github code](https://github.com/hustvl/YOLOP/tree/main/toolkits/deploy).\n",
        "\n",
        "\n",
        "### Load From PyTorch Hub\n",
        "This example loads the pretrained **YOLOP** model and passes an image for inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "f8533718",
      "metadata": {
        "id": "f8533718"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in C:\\Users\\micha/.cache\\torch\\hub\\hustvl_yolop_main\n",
            "C:\\Users\\micha/.cache\\torch\\hub\\hustvl_yolop_main\\hubconf.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(path, map_location= device)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# load model\n",
        "model = torch.hub.load('hustvl/yolop', 'yolop', pretrained=True)\n",
        "\n",
        "#inference\n",
        "img = torch.randn(1,3,640,640)\n",
        "det_out, da_seg_out,ll_seg_out = model(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2642962",
      "metadata": {
        "id": "b2642962"
      },
      "source": [
        "### Citation\n",
        "\n",
        "See for more detail in [github code](https://github.com/hustvl/YOLOP) and [arxiv paper](https://arxiv.org/abs/2108.11250).\n",
        "\n",
        "If you find our paper and code useful for your research, please consider giving a star and citation:"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv-torch251",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
